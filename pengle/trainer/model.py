import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import lightgbm as lgb
import optuna
import os
import uuid
import pickle
from statistics import mean

from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from pengle.storage.storage import save_model, output_csv


def train_optuna_for_lightgbm(train_dataset, feature_pipeline,
                              n_trials=100, n_splits=5, random_state=19930201, shuffle=True, num_boost_round=500):
    """optunaを用いてlightgbmのパラメータの最適化を行って、最適なパラメータを返す関数

    Arguments:
        train_dataset {[type]} -- [description]

    Returns:
        dict -- チューニングによって取得した最適なパラメータ
    """

    def objectives(trial):
        # 試行にUUIDを設定
        trial_uuid = str(uuid.uuid4())
        trial.set_user_attr("uuid", trial_uuid)

        params = get_default_parameter_suggestions(trial)

        if params['boosting_type'] == 'dart':
            params['drop_rate'] = trial.suggest_loguniform('drop_rate', 1e-8, 1.0)
            params['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)
        if params['boosting_type'] == 'goss':
            params['top_rate'] = trial.suggest_uniform('top_rate', 0.0, 1.0)
            params['other_rate'] = trial.suggest_uniform('other_rate', 0.0, 1.0 - params['top_rate'])

        cross_validator = StratifiedKFold(n_splits=n_splits,
                                          random_state=random_state,
                                          shuffle=shuffle)

        error_trains = []
        error_validations = []
        cv = cross_validator.split(train_dataset["data"], train_dataset["target"])
        x = train_dataset["data"]
        y = np.array(train_dataset["target"])

        for train_index, validation_index in cv:
            X_train_dataset, X_valid_dataset = split_dataset(
                train_dataset, train_index=train_index, valid_index=validation_index)
            x_train, x_validation = feature_pipeline.run(X_train_dataset, X_valid_dataset)
            y_train, y_validation = X_train_dataset.target, X_valid_dataset.target

            x_train_columns = x_train.columns
            trn_data = lgb.Dataset(x_train.values,
                                   label=y_train)
            val_data = lgb.Dataset(x_validation.values,
                                   label=y_validation)

            # 枝刈りありの訓練
            pruning_callback = optuna.integration.LightGBMPruningCallback(trial, "binary_logloss")
            model = lgb.train(params, trn_data, num_boost_round=num_boost_round, verbose_eval=False,
                              valid_sets=val_data, callbacks=[pruning_callback])
            del trn_data
            del val_data
            y_pred_valid = model.predict(x_validation, num_iteration=model.best_iteration)

            # 訓練、テスト誤差
            y_pred_train = np.rint(model.predict(x_train))
            y_pred_test = np.rint(model.predict(x_validation))
            error_train = 1.0 - accuracy_score(y_train, y_pred_train)
            error_test = 1.0 - accuracy_score(y_validation, y_pred_test)
            error_trains.append(error_train)
            error_validations.append(error_test)

        # エラー率の記録
        trial.set_user_attr("mean_train_error", mean(error_trains))
        trial.set_user_attr("mean_test_error", mean(error_validations))

        return mean(error_validations)

    study = optuna.create_study()
    study.optimize(objectives, n_trials=n_trials)

    print(study.best_params)
    print(study.best_value)

    # best_paramsにはuser_attrは表示されないのでtrialから呼ぶ（dict形式で記録されている）
    print(study.best_trial.user_attrs)

    return study.best_params


def get_default_parameter_suggestions(trial):
    """
    Get parameter sample for Boosting (like XGBoost, LightGBM)
    reference:
        https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1

    Args:
        trial(trial.Trial):

    Returns:
        dict: parameter sample generated by trial object
    """
    return {
        'boosting_type': 'gbdt',
        'metric': 'binary',
        # L2 正則化
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e3),
        # L1 正則化
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e3),
        # 弱学習木ごとに使う特徴量の割合
        # 0.5 だと全体のうち半分の特徴量を最初に選んで, その範囲内で木を成長させる
        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, .1),
        # 学習データ全体のうち使用する割合
        # colsample とは反対に row 方向にサンプルする
        'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),
        # 木の最大の深さ
        # たとえば 5 の時各弱学習木の各データに対するルールは、最大でも5に制限される.
        'max_depth': trial.suggest_categorical('max_depth', [3, 5, 6, 7, 8]),
        # 末端ノードに含まれる最小のサンプル数
        # これを下回るような分割は作れなくなるため, 大きく設定するとより全体の傾向でしか分割ができなくなる
        # [NOTE]: 数であるのでデータセットの大きさ依存であることに注意
        'min_child_weight': trial.suggest_int('min_child_weight', 5, 40),
        'verbose': 0
    }


def get_default_parametor(objective):
    if objective == 'regression':
        params = {
            'task': 'train',
            'boosting_type': 'gbdt',
            'objective': 'regression',
            'metric': {'l2'},
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': 0
        }
    elif objective == 'classification':
        params = {'learning_rate': 0.03,
                  'objective': 'binary',
                  'metric': 'binary_logloss',
                  'class_weight': 'balanced',
                  'num_leaves': 31,
                  'verbose': 1,
                  'subsample': 0.99,
                  'colsample_bytree': 0.99,
                  'random_state': 42,
                  'max_depth': 15,
                  'lambda_l2': 0.02085548700474218,
                  'lambda_l1': 0.004107624022751344,
                  'bagging_fraction': 0.7934712636944741,
                  'feature_fraction': 0.686612409641711,
                  'min_child_samples': 21
                  }

    return params


def train_cv(x, y, lgb_params,
             number_of_folds=5, 
             evaluation_metric='auc', 
             save_feature_importances=True, 
             early_stopping_rounds=50, 
             num_round=50,
             random_state=19930201,
             shuffle=True,
             drop_columns=[],
             categorical_columns=[],
             score_metric='auc'):
    """Cross Validation用の関数

    Arguments:
        x {DataFrame} -- 学習用の特徴量のDataFrame
        y {list} -- 学習用の目的変数のリスト
        lgb_params {dict} -- lightgbmのパラメータの辞書

    Keyword Arguments:
        number_of_folds {int} -- StratifiedKFoldのsplitの数 (default: {5})
        evaluation_metric {str} -- [description] (default: {'auc'})
        save_feature_importances {bool} -- [description] (default: {True})
        early_stopping_rounds {int} -- [description] (default: {50})
        num_round {int} -- [description] (default: {50})
        random_state {int} -- [description] (default: {19930201})
        shuffle {bool} -- [description] (default: {True})
        drop_columns {list} -- [description] (default: {[]})
        categorical_columns {list} -- [description] (default: {[]})
        score_metric {str} -- [description] (default: {'auc'})

    Returns:
        [type] -- [description]
    """

    cross_validator = StratifiedKFold(n_splits=number_of_folds,
                                      random_state=random_state,
                                      shuffle=shuffle)

    validation_scores = []
    models = []
    feature_importance_df = pd.DataFrame()
    y = np.array(y)
    cv = cross_validator.split(x, y)
    for fold_index, (train_index, validation_index) in enumerate(cv):
        x_train, x_validation = x.iloc[train_index], x.iloc[validation_index]
        y_train, y_validation = y[train_index], y[validation_index]

        if drop_columns:
            x_train.drop(drop_columns, axis=1, inplace=True)
            x_validation.drop(drop_columns, axis=1, inplace=True)

        x_train_columns = x_train.columns
        trn_data = lgb.Dataset(x_train,
                               label=y_train,
                               categorical_feature=categorical_columns)
        del x_train
        del y_train
        val_data = lgb.Dataset(x_validation,
                               label=y_validation,
                               categorical_feature=categorical_columns)
        model = lgb.train(lgb_params,
                          trn_data,
                          num_round,
                          valid_sets=[trn_data, val_data],
                          verbose_eval=100,
                          early_stopping_rounds=early_stopping_rounds
                          )

        models.append(model)

        predictions = model.predict(x_validation, num_iteration=model.best_iteration)

        score = calc_score(score_metric, y_validation, predictions)

        validation_scores.append(score)

        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = x_train_columns
        fold_importance_df["importance"] = model.feature_importance(importance_type='gain')
        fold_importance_df["fold"] = fold_index + 1
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)

    if save_feature_importances:
        save_importance_graph(feature_importance_df)

    score = sum(validation_scores) / len(validation_scores)
    return models, score


def calc_score(score_metric, y, predictions):
    if score_metric == 'auc':
        false_positive_rate, recall, thresholds = metrics.roc_curve(y, predictions)
        score = metrics.auc(false_positive_rate, recall)
    elif score_metric == 'accuracy':
        predictions = [1 if predictions[i] >= 0.5 else 0 for i in range(len(predictions))]
        score = metrics.accuracy_score(y, predictions)
    elif score_metric == 'f1':
        predictions = [1 if predictions[i] >= 0.5 else 0 for i in range(len(predictions))]
        score = metrics.f1_score(y, predictions)
    elif score_metric == 'precision':
        predictions = [1 if predictions[i] >= 0.5 else 0 for i in range(len(predictions))]
        score = metrics.precision_score(y, predictions)
    elif score_metric == 'recall':
        predictions = [1 if predictions[i] >= 0.5 else 0 for i in range(len(predictions))]
        score = metrics.recall_score(y, predictions)
    elif score_metric == 'mae':
        score = metrics.mean_absolute_error(y, predictions)
    elif score_metric == 'mse':
        score = metrics.mean_squared_error(y, predictions)
    elif score_metric == 'msle':
        score = metrics.mean_squared_log_error(y, predictions)
    return score


def save_importance_graph(feature_importance_df):
    cols = (feature_importance_df[["feature", "importance"]]
            .groupby("feature")
            .mean()
            .sort_values(by="importance", ascending=False)[:1000].index)

    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]

    plt.figure(figsize=(14, 25))
    sns.barplot(x="importance",
                y="feature",
                data=best_features.sort_values(by="importance",
                                                ascending=False))
    plt.title('LightGBM Features (avg over folds)')
    plt.tight_layout()
    plt.savefig('lgbm_importances.png')

    # mean_gain = feature_importances[['gain', 'feature']].groupby('feature').mean()
    # feature_importances['mean_gain'] = feature_importances['feature'].map(mean_gain['gain'])
    #
    # temp = feature_importances.sort_values('mean_gain', ascending=False)
    best_features.sort_values(by="importance", ascending=False) \
        .groupby("feature") \
        .mean() \
        .sort_values(by="importance", ascending=False) \
        .to_csv('feature_importances_new.csv', index=True)
    return best_features


def train(x, y, lgb_params,
          train_size=100,
          early_stopping_rounds=50, 
          num_round=50,
          random_state=19930201,
          drop_columns=[],
          categorical_columns=[]):
    x_train, x_validation = x.iloc[:train_size, :], x.iloc[train_size:, :]
    y_train, y_validation = y[:train_size], y[train_size:]

    if drop_columns:
        x_train.drop(drop_columns, axis=1, inplace=True)
        x_validation.drop(drop_columns, axis=1, inplace=True)

    x_train_columns = x_train.columns
    trn_data = lgb.Dataset(x_train,
                           label=y_train,
                           categorical_feature=categorical_columns)
    del x_train
    del y_train
    val_data = lgb.Dataset(x_validation,
                           label=y_validation,
                           categorical_feature=categorical_columns)
    model = lgb.train(lgb_params,
                      trn_data,
                      num_round,
                      valid_sets=[trn_data, val_data],
                      verbose_eval=100,
                      early_stopping_rounds=early_stopping_rounds
                      )

    predictions = model.predict(x_validation, num_iteration=model.best_iteration)
    save_model(model, './output/models/')


def train_and_predict_test(X, y, X_test, lgb_params, model_name, id_col, predict_col_name,
                           predict_method='binary',
                           train_size=600,
                           early_stopping_rounds=50, 
                           num_round=50,
                           random_state=19930201,
                           drop_columns=[],
                           categorical_columns=[]):
    X_train, X_validation = X.iloc[:train_size, :], X.iloc[train_size:, :]
    y_train, y_validation = y[:train_size], y[train_size:]

    if drop_columns:
        X_train.drop(drop_columns, axis=1, inplace=True)
        X_validation.drop(drop_columns, axis=1, inplace=True)
        X_test.drop(drop_columns, axis=1, inplace=True)

    x_train_columns = X_train.columns

    train_data = lgb.Dataset(X_train,
                             label=y_train,
                             categorical_feature=categorical_columns)
    del X_train
    del y_train
    val_data = lgb.Dataset(X_validation,
                           label=y_validation,
                           categorical_feature=categorical_columns)

    model = lgb.train(lgb_params,
                      train_data,
                      num_round,
                      valid_sets=[train_data, val_data],
                      verbose_eval=100,
                      early_stopping_rounds=early_stopping_rounds
                      )

    predictions = model.predict(X_test, num_iteration=model.best_iteration)
    if predict_method == 'binary':
        predictions = [1 if predictions[i] >= 0.5 else 0 for i in range(len(predictions))]

    df_submit = pd.DataFrame()
    df_submit[id_col.name] = id_col
    df_submit[predict_col_name] = predictions
    save_model(model, './output/models/' + model_name + '_submit.ftr')
    output_csv(df_submit, './output/submits/' + model_name + '_submit.csv')
